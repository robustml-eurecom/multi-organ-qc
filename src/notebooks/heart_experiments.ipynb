{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/marciano/experiments/one-to-rule-them-all/src/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/marciano/experiments/one-to-rule-them-all\n"
     ]
    }
   ],
   "source": [
    "%cd /data/marciano/experiments/one-to-rule-them-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/marciano/experiments/one-to-rule-them-all\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from src.utils.utilsAcdc import generate_patient_info, preprocess\n",
    "\n",
    "patient_info = generate_patient_info(\"data/heart/training/\", patient_ids=range(1, 101))\n",
    "patient_info = {**patient_info, **generate_patient_info(\"data/heart/testing/\", patient_ids=range(101, 151))}\n",
    "\n",
    "if not os.path.exists(\"preprocessed/heart\"):\n",
    "    os.makedirs(\"preprocessed/heart/\")\n",
    "np.save(os.path.join(\"preprocessed/heart/\", \"patient_info\"), patient_info)\n",
    "\n",
    "#spacings = [patient_info[id][\"spacing\"] for id in range(1, 101)]\n",
    "#spacing_target = np.percentile(np.vstack(spacings), 50, 0)\n",
    "spacing_target = [10, 1.25, 1.25]\n",
    "\n",
    "if not os.path.exists(\"preprocessed/heart/training/\"):\n",
    "    os.makedirs(\"preprocessed/heart/training/\")\n",
    "preprocess(\n",
    "    range(1,101), patient_info, spacing_target,\n",
    "    \"data/heart/training\", \"preprocessed/heart/training\",\n",
    "    lambda folder, id: os.path.join(folder, 'patient{:03d}'.format(id)),\n",
    "    lambda patient_info, id, phase: \"patient{:03d}_frame{:02d}_gt.nii.gz\".format(id, patient_info[id][phase])\n",
    ")\n",
    "\n",
    "for model in os.listdir(\"data/heart/predictions/\"):\n",
    "    if not os.path.exists(\"preprocessed/heart/predictions/{}\".format(model)):\n",
    "        os.makedirs(\"preprocessed/heart/predictions/{}\".format(model))\n",
    "    preprocess(\n",
    "        range(101,151), patient_info, spacing_target,\n",
    "        \"data/heart/predictions/{}\".format(model), \"preprocessed/heart/predictions/{}\".format(model),\n",
    "        lambda folder, id: folder,\n",
    "        lambda patient_info, id, phase: \"patient{:03d}_{}.nii.gz\".format(id,phase)\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision\n",
    "from src.utils.utilsAcdc import AddPadding, CenterCrop, OneHot, ToTensor, MirrorTransform, SpatialTransform\n",
    "\n",
    "ids = random.sample(range(1, 101), 100)\n",
    "train_ids = ids[:80]\n",
    "val_ids = ids[80:]\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    AddPadding((256,256)),\n",
    "    CenterCrop((256,256)),\n",
    "    OneHot(),\n",
    "    ToTensor()\n",
    "])\n",
    "transform_augmentation = torchvision.transforms.Compose([\n",
    "    MirrorTransform(),\n",
    "    SpatialTransform(patch_size=(256,256), angle_x=(-np.pi/6,np.pi/6), scale=(0.7,1.4), random_crop=True),\n",
    "    OneHot(),\n",
    "    ToTensor()\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "importlib.reload(sys.modules['CA'])\n",
    "\n",
    "import torch\n",
    "from src.ConvAE.basic_model import AE\n",
    "from src.ConvAE.utils import hyperparameter_tuning, plot_history\n",
    "from utils.utilsAcdc import ACDCDataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#the following cell allows the user to tune his own hyperparameters. In case you wanna try our hyperparameters first, just skip this cell.\n",
    "\n",
    "#this is a list of possible values being tested for each hyperparameter.\n",
    "parameters = {\n",
    "    \"DA\": [True, False], #data augmentation\n",
    "    \"latent_size\": [100, 500], #size of the latent space of the autoencoder\n",
    "    \"BATCH_SIZE\": [8, 16, 4],\n",
    "    \"optimizer\": [torch.optim.Adam],\n",
    "    \"lr\": [2e-4, 1e-4, 1e-3],\n",
    "    \"weight_decay\": [1e-5],\n",
    "    \"tuning_epochs\": [5, 10], #number of epochs each configuration is run for\n",
    "    \"functions\": [[\"GDLoss\", \"MSELoss\"], [\"GDLoss\"], [\"BKGDLoss\", \"BKMSELoss\"]], #list of loss functions to be evaluated. BK stands for \"background\", which is a predominant and not compulsory class (it can lead to a dumb local minimum retrieving totally black images).\n",
    "    \"settling_epochs_BKGDLoss\": [10, 0], #during these epochs BK has half the weight of LV, RV and MYO in the evaluation of BKGDLoss\n",
    "    \"settling_epochs_BKMSELoss\": [10, 0], #during these epochs BK has half the weight of LV, RV and MYO in the evaluation of BKMSELoss\n",
    "}\n",
    "\n",
    "#this is a list of rules cutting out some useless combinations of hyperparameters from the tuning process.\n",
    "rules = [\n",
    "    '\"settling_epochs_BKGDLoss\" == 0 or \"BKGDLoss\" in \"functions\"',\n",
    "    '\"settling_epochs_BKMSELoss\" == 0 or \"BKMSELoss\" in \"functions\"',\n",
    "    '\"BKGDLoss\" not in \"functions\" or \"settling_epochs_BKGDLoss\" <= \"tuning_epochs\"',\n",
    "    '\"BKMSELoss\" not in \"functions\" or \"settling_epochs_BKMSELoss\" <= \"tuning_epochs\"',\n",
    "    #'\"BKGDLoss\" not in \"functions\" or \"settling_epochs_BKGDLoss\" >= \"settling_epochs_BKMSELoss\"'\n",
    "]\n",
    "\n",
    "optimal_parameters = hyperparameter_tuning(\n",
    "    parameters,\n",
    "    ACDCDataLoader(\"preprocessed/heart/training\", patient_ids=train_ids, batch_size=None, transform=None),\n",
    "    ACDCDataLoader(\"preprocessed/heart/training\", patient_ids=val_ids, batch_size=None, transform=None),\n",
    "    transform, transform_augmentation,\n",
    "    rules,\n",
    "    fast=True) #very important parameter. When False, all combinations are tested to return the one retrieving the maximum DSC. When True, the first combination avoiding dumb local minima is returned.\n",
    "\n",
    "np.save(os.path.join(\"preprocessed/heart/\", \"optimal_parameters\"), optimal_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_your_own_parameters = False\n",
    "\n",
    "if upload_your_own_parameters:\n",
    "    optimal_parameters = np.load(os.path.join(\"preprocessed/heart\", \"optimal_parameters.npy\"), allow_pickle=True).item()\n",
    "else:\n",
    "    optimal_parameters = {\n",
    "        \"BATCH_SIZE\": 8,\n",
    "        \"DA\": False,\n",
    "        \"latent_size\": 100,\n",
    "        \"optimizer\": torch.optim.Adam,\n",
    "        \"lr\": 2e-4,\n",
    "        \"weight_decay\": 1e-5,\n",
    "        \"functions\": [\"BKGDLoss\", \"BKMSELoss\"],\n",
    "        \"settling_epochs_BKGDLoss\": 10,\n",
    "        \"settling_epochs_BKMSELoss\": 0\n",
    "    }\n",
    "    np.save(os.path.join(\"preprocessed/heart\", \"optimal_parameters\"), optimal_parameters)\n",
    "\n",
    "assert optimal_parameters is not None, \"Be sure to continue with a working set of hyperparameters\"\n",
    "\n",
    "BATCH_SIZE = optimal_parameters[\"BATCH_SIZE\"]\n",
    "DA = optimal_parameters[\"DA\"]\n",
    "\n",
    "ae = AE(**optimal_parameters).to(device)\n",
    "\n",
    "ckpt = None\n",
    "if ckpt is not None:\n",
    "    ckpt = torch.load(ckpt)\n",
    "    ae.load_state_dict(ckpt[\"AE\"])\n",
    "    ae.optimizer.load_state_dict(ckpt[\"AE_optim\"])\n",
    "    start = ckpt[\"epoch\"]+1\n",
    "else:\n",
    "    start = 0\n",
    "\n",
    "print(ae)\n",
    "\n",
    "plot_history(\n",
    "    ae.training_routine(\n",
    "        range(start, 500),\n",
    "        ACDCDataLoader(\"preprocessed/heart/training\", patient_ids=train_ids, batch_size=BATCH_SIZE, transform=transform_augmentation if DA else transform),\n",
    "        ACDCDataLoader(\"preprocessed/heart/training\", patient_ids=val_ids, batch_size=BATCH_SIZE, transform=transform),\n",
    "        \"checkpoints/\"\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FORE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
