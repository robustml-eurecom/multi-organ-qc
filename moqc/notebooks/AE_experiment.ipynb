{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from skimage.util import random_noise\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.networks.nets import AutoEncoder\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstD,\n",
    "    Compose,\n",
    "    LoadImageD,\n",
    "    RandFlipD,\n",
    "    RandRotateD,\n",
    "    RandZoomD,\n",
    "    ScaleIntensityD,\n",
    "    EnsureTypeD,\n",
    "    Lambda,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "from models.ConvAE import ConvolutionalAutoencoder\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "set_determinism(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small visualisation function\n",
    "import nibabel as nib\n",
    "\n",
    "def plot_ims(ims, shape=None, figsize=(10, 10), titles=None):\n",
    "    shape = (1, len(ims)) if shape is None else shape\n",
    "    plt.subplots(*shape, figsize=figsize)\n",
    "    for i, im in enumerate(ims):\n",
    "        plt.subplot(*shape, i + 1)\n",
    "        if isinstance(im, str): im = nib.load(im).get_fdata().astype(int) \n",
    "        im = plt.imread(im) if isinstance(im, str) else np.squeeze(im)\n",
    "        plt.imshow(im, cmap=\"gray\")\n",
    "        if titles is not None:\n",
    "            plt.title(titles[i])\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organ = 'spleen'\n",
    "DATA_PATH = os.path.join(\"data\", organ)\n",
    "mask_path = os.path.join(DATA_PATH, \"structured\")\n",
    "\n",
    "all_filenames = [os.path.join(mask_path, filename, \"mask.nii.gz\") for filename in os.listdir(mask_path)]\n",
    "random.shuffle(all_filenames)\n",
    "\n",
    "# Visualise a few of them\n",
    "rand_images = np.random.choice(all_filenames, 8, replace=False)\n",
    "plot_ims(rand_images, shape=(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frac = 0.2\n",
    "num_test = int(len(all_filenames) * test_frac)\n",
    "num_train = len(all_filenames) - num_test\n",
    "train_datadict = [{\"im\": fname} for fname in all_filenames[:num_train]]\n",
    "test_datadict = [{\"im\": fname} for fname in all_filenames[-num_test:]]\n",
    "print(f\"total number of images: {len(all_filenames)}\")\n",
    "print(f\"number of images for training: {len(train_datadict)}\")\n",
    "print(f\"number of images for testing: {len(test_datadict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseLambda = Lambda(\n",
    "    lambda d: {\n",
    "        \"orig\": d[\"im\"],\n",
    "        \"gaus\": torch.tensor(random_noise(d[\"im\"], mode=\"gaussian\"), dtype=torch.float32),\n",
    "        \"s&p\": torch.tensor(random_noise(d[\"im\"], mode=\"s&p\", salt_vs_pepper=0.1)),\n",
    "    }\n",
    ")\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImageD(keys=[\"im\"]),\n",
    "        EnsureChannelFirstD(keys=[\"im\"]),\n",
    "        ScaleIntensityD(keys=[\"im\"]),\n",
    "        RandRotateD(keys=[\"im\"], range_x=np.pi / 12, prob=0.5, keep_size=True),\n",
    "        RandFlipD(keys=[\"im\"], spatial_axis=0, prob=0.5),\n",
    "        EnsureTypeD(keys=[\"im\"]),\n",
    "        NoiseLambda,\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = Compose(\n",
    "    [\n",
    "        LoadImageD(keys=[\"im\"]),\n",
    "        EnsureChannelFirstD(keys=[\"im\"]),\n",
    "        ScaleIntensityD(keys=[\"im\"]),\n",
    "        EnsureTypeD(keys=[\"im\"]),\n",
    "        NoiseLambda,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_workers = 8\n",
    "\n",
    "train_ds = CacheDataset(train_datadict, train_transforms, num_workers=num_workers)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_ds = CacheDataset(test_datadict, test_transforms, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image original and its degraded versions\n",
    "def get_single_im(ds):\n",
    "    loader = torch.utils.data.DataLoader(ds, batch_size=1, num_workers=8, shuffle=True)\n",
    "    itera = iter(loader)\n",
    "    return next(itera)\n",
    "\n",
    "data = get_single_im(train_ds)\n",
    "plot_ims([data[\"orig\"], data[\"gaus\"], data[\"s&p\"]], titles=[\"orig\", \"Gaussian\", \"s&p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dict_key_for_training, max_epochs=10, learning_rate=1e-3):\n",
    "    model = AutoEncoder(\n",
    "        spatial_dims=2,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        channels=(8, 16, 32, 32, 64, 64),\n",
    "        strides=(2, 2, 2, 1, 2, 1),\n",
    "        inter_channels=(100, ),\n",
    "        inter_dilations=(2, )\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create loss fn and optimiser\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), learning_rate)\n",
    "\n",
    "    epoch_loss_values = []\n",
    "\n",
    "    t = trange(max_epochs, desc=f\"{dict_key_for_training} -- epoch 0, avg loss: inf\", leave=True)\n",
    "    for epoch in t:\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs = batch_data[dict_key_for_training].to(device).squeeze(2)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs.to(device).squeeze(2), batch_data[\"orig\"].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        t.set_description(f\"{dict_key_for_training} -- epoch {epoch + 1}\" + f\", average loss: {epoch_loss:.4f}\")\n",
    "    return model, epoch_loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 500\n",
    "training_types = [\"orig\", \"gaus\", \"s&p\"]\n",
    "models = []\n",
    "epoch_losses = []\n",
    "for training_type in training_types:\n",
    "    model, epoch_loss = train(training_type, max_epochs=max_epochs)\n",
    "    models.append(model)\n",
    "    epoch_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "for y, label in zip(epoch_losses, training_types):\n",
    "    x = list(range(1, len(y) + 1))\n",
    "    (line,) = plt.plot(x, y)\n",
    "    line.set_label(label)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_single_im(test_ds)\n",
    "\n",
    "recons = []\n",
    "for model, training_type in zip(models, training_types):\n",
    "    im = data[training_type]\n",
    "    recon = torch.where(model(im.to(device).squeeze(2))> 0.3, 1, 0).detach().cpu()\n",
    "    recons.append(np.squeeze(recon))\n",
    "\n",
    "plot_ims(\n",
    "    [data[\"orig\"], data[\"gaus\"], data[\"s&p\"]] + recons,\n",
    "    titles=[\"orig\", \"Gaussian\", \"S&P\"] + [\"recon w/\\n\" + x for x in training_types],\n",
    "    shape=(2, len(training_types)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FORE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
